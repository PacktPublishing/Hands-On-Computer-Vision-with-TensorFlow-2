{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Influential Classification Tools - Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last notebook covering Chapter 4, we will demonstrate how transfer learning can be achieved with Keras. More precisely, we will present how Keras Applications pre-trained on rich datasets can be reused for new tasks. Unlike Notebook [4-3](./ch4_notebook_3_resnet_from_keras_app.ipynb) where we instantiated a ResNet-50 from Keras-App with random parameters, we will this time ask Keras to fetch for us the parameters pre-trained on ImageNet. This will give us the opportunity to test different types of transfer learning; i.e. **_freezing_** or **_fine-tuning_** the feature-extractor layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_imagenet import (\n",
    "    tiny_imagenet, _training_augmentation_fn, \n",
    "    IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, NUM_CLASSES)\n",
    "\n",
    "TINY_IMAGENET_ROOT_FOLDER = os.path.expanduser('~/datasets/tiny-imagenet-200/')\n",
    "\n",
    "NUM_TRAINING_IMAGES = 500 * NUM_CLASSES\n",
    "NUM_VAL_IMAGES = 50 * NUM_CLASSES\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "train_steps_per_epoch = NUM_TRAINING_IMAGES // batch_size\n",
    "val_steps_per_epoch = NUM_VAL_IMAGES // batch_size\n",
    "\n",
    "# Like in previous notebooks, we actually resize the Tiny-ImageNet images to ImageNet commonly-used dimensions:\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with Frozen Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, we will use the ResNet-50 from Keras Application, pre-trained on ImageNet, as a feature extractor, and build a new classifier for Tiny-ImageNet on top (c.f. Chapter 4). We will then illustrate the first transfer learning use-case presented in the book, i.e., completely freezing the feature extractor and only training the new dense layers on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a New Classifier from Pre-trained Keras Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build our model, a ResNet-50 solution to predict from the 200 classes of Tiny-ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first use Keras Applications to instantiate a network with pre-trained weights, but without any top layers (i.e., without the final dense layers leading to predictions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/envs/main/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "resnet50_feature_extractor = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False, weights='imagenet', \n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "# resnet50_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earler, we then _freeze_ this feature extractor; i.e., we make the layers of this network non-trainable, as we want to preserve the knowledge this ResNet obtained by being trained on ImageNet, a much richer dataset.\n",
    "\n",
    "However, while we want to preserve the feature-extracting layers (i.e., the convolutional layers making most of ResNet), **we should be careful not to freeze some other layers like the regularization ones**. Layers like the _batch-normalization_ ones (added after most of the convolutions in ResNet architectures) have some trainable parameters (c.f. Chapter 3) which tend to become too dataset-specific. It is often recommended not to freeze such layers and let them adapt to the new task/dataset. Therefore, we check the layers' type before freezing them or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer conv1: trainable = False\n",
      "Layer res2a_branch2a: trainable = False\n",
      "Layer res2a_branch2b: trainable = False\n",
      "Layer res2a_branch2c: trainable = False\n",
      "Layer res2a_branch1: trainable = False\n",
      "Layer res2b_branch2a: trainable = False\n",
      "Layer res2b_branch2b: trainable = False\n",
      "Layer res2b_branch2c: trainable = False\n",
      "Layer res2c_branch2a: trainable = False\n",
      "Layer res2c_branch2b: trainable = False\n",
      "Layer res2c_branch2c: trainable = False\n",
      "Layer res3a_branch2a: trainable = False\n",
      "Layer res3a_branch2b: trainable = False\n",
      "Layer res3a_branch2c: trainable = False\n",
      "Layer res3a_branch1: trainable = False\n",
      "Layer res3b_branch2a: trainable = False\n",
      "Layer res3b_branch2b: trainable = False\n",
      "Layer res3b_branch2c: trainable = False\n",
      "Layer res3c_branch2a: trainable = False\n",
      "Layer res3c_branch2b: trainable = False\n",
      "Layer res3c_branch2c: trainable = False\n",
      "Layer res3d_branch2a: trainable = False\n",
      "Layer res3d_branch2b: trainable = False\n",
      "Layer res3d_branch2c: trainable = False\n",
      "Layer res4a_branch2a: trainable = False\n",
      "Layer res4a_branch2b: trainable = False\n",
      "Layer res4a_branch2c: trainable = False\n",
      "Layer res4a_branch1: trainable = False\n",
      "Layer res4b_branch2a: trainable = False\n",
      "Layer res4b_branch2b: trainable = False\n",
      "Layer res4b_branch2c: trainable = False\n",
      "Layer res4c_branch2a: trainable = False\n",
      "Layer res4c_branch2b: trainable = False\n",
      "Layer res4c_branch2c: trainable = False\n",
      "Layer res4d_branch2a: trainable = False\n",
      "Layer res4d_branch2b: trainable = False\n",
      "Layer res4d_branch2c: trainable = False\n",
      "Layer res4e_branch2a: trainable = False\n",
      "Layer res4e_branch2b: trainable = False\n",
      "Layer res4e_branch2c: trainable = False\n",
      "Layer res4f_branch2a: trainable = False\n",
      "Layer res4f_branch2b: trainable = False\n",
      "Layer res4f_branch2c: trainable = False\n",
      "Layer res5a_branch2a: trainable = False\n",
      "Layer res5a_branch2b: trainable = False\n",
      "Layer res5a_branch2c: trainable = False\n",
      "Layer res5a_branch1: trainable = False\n",
      "Layer res5b_branch2a: trainable = False\n",
      "Layer res5b_branch2b: trainable = False\n",
      "Layer res5b_branch2c: trainable = False\n",
      "Layer res5c_branch2a: trainable = False\n",
      "Layer res5c_branch2b: trainable = False\n",
      "Layer res5c_branch2c: trainable = False\n"
     ]
    }
   ],
   "source": [
    "for layer in resnet50_feature_extractor.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        layer.trainable = False\n",
    "        print(\"Layer {}: trainable = False\".format(layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add on top of this network the trainable layers to make predictions from the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = resnet50_feature_extractor.output\n",
    "avg_pool = GlobalAveragePooling2D(data_format='channels_last')(features)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(avg_pool)\n",
    "\n",
    "resnet50_freeze = Model(resnet50_feature_extractor.input, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we reuse the functions we implemented in a previous [notebook](./ch4_notebook_1_data_preparation.ipynb) to set up the input pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, class_ids, class_readable_labels = tiny_imagenet(\n",
    "    phase='train', shuffle=True, batch_size=batch_size, num_epochs=num_epochs, wrap_for_estimator=False,\n",
    "    augmentation_fn=_training_augmentation_fn, root_folder=TINY_IMAGENET_ROOT_FOLDER,\n",
    "    resize_to=[IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "val_images, val_labels, _, _ = tiny_imagenet(\n",
    "    phase='val', shuffle=False, batch_size=batch_size, num_epochs=None, wrap_for_estimator=False,\n",
    "    augmentation_fn=None, root_folder=TINY_IMAGENET_ROOT_FOLDER,\n",
    "    resize_to=[IMG_HEIGHT, IMG_WIDTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the training script itself is purely copy-pasted from previous notebooks (we invite our readers to check them if details are needed). Indeed, with the loading of the pre-trained weights and the freezing of the desired layers already covered, the resulting model can be trained like any others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3125/3125 [==============================] - 602s 193ms/step - loss: 1.5493 - sparse_categorical_accuracy: 0.6176 - sparse_top_5_categorical_accuracy: 0.8423 - val_loss: 1.1757 - val_sparse_categorical_accuracy: 0.7020 - val_sparse_top_5_categorical_accuracy: 0.8953\n",
      "Epoch 2/30\n",
      "3125/3125 [==============================] - 597s 191ms/step - loss: 0.8983 - sparse_categorical_accuracy: 0.7590 - sparse_top_5_categorical_accuracy: 0.9290 - val_loss: 1.0782 - val_sparse_categorical_accuracy: 0.7228 - val_sparse_top_5_categorical_accuracy: 0.9097\n",
      "Epoch 3/30\n",
      "3125/3125 [==============================] - 596s 191ms/step - loss: 0.7358 - sparse_categorical_accuracy: 0.7957 - sparse_top_5_categorical_accuracy: 0.9488 - val_loss: 1.0296 - val_sparse_categorical_accuracy: 0.7407 - val_sparse_top_5_categorical_accuracy: 0.9164\n",
      "Epoch 4/30\n",
      "3125/3125 [==============================] - 596s 191ms/step - loss: 0.6333 - sparse_categorical_accuracy: 0.8204 - sparse_top_5_categorical_accuracy: 0.9603 - val_loss: 1.0288 - val_sparse_categorical_accuracy: 0.7434 - val_sparse_top_5_categorical_accuracy: 0.9176\n",
      "Epoch 5/30\n",
      "3125/3125 [==============================] - 612s 196ms/step - loss: 0.5507 - sparse_categorical_accuracy: 0.8413 - sparse_top_5_categorical_accuracy: 0.9685 - val_loss: 1.0430 - val_sparse_categorical_accuracy: 0.7443 - val_sparse_top_5_categorical_accuracy: 0.9175\n",
      "Epoch 6/30\n",
      "3125/3125 [==============================] - 596s 191ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8556 - sparse_top_5_categorical_accuracy: 0.9745 - val_loss: 1.0587 - val_sparse_categorical_accuracy: 0.7441 - val_sparse_top_5_categorical_accuracy: 0.9165\n",
      "Epoch 7/30\n",
      "3125/3125 [==============================] - 596s 191ms/step - loss: 0.4455 - sparse_categorical_accuracy: 0.8691 - sparse_top_5_categorical_accuracy: 0.9784 - val_loss: 1.0671 - val_sparse_categorical_accuracy: 0.7469 - val_sparse_top_5_categorical_accuracy: 0.9166\n",
      "Epoch 8/30\n",
      " 685/3125 [=====>........................] - ETA: 7:29 - loss: 0.3022 - sparse_categorical_accuracy: 0.9099 - sparse_top_5_categorical_accuracy: 0.9889"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "sparse_top_5_categorical_accuracy = functools.partial(\n",
    "    tf.keras.metrics.sparse_top_k_categorical_accuracy, k=5)\n",
    "sparse_top_5_categorical_accuracy.__name__ = 'sparse_top_5_categorical_accuracy'\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "\n",
    "model_dir = './models/resnet_keras_app_transfer_learning_freeze'\n",
    "callbacks = [\n",
    "    # Callback to log the graph, losses and metrics into TensorBoard:\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=model_dir, histogram_freq=0, write_graph=True),\n",
    "    # Callback to save the model (e.g., every 5 epochs), specifying the epoch and val-loss in the filename:\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'weights-epoch{epoch:02d}-loss{val_loss:.2f}.h5'), period=5)\n",
    "]\n",
    "\n",
    "\n",
    "resnet50_freeze.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "                 metrics=['sparse_categorical_accuracy', sparse_top_5_categorical_accuracy])\n",
    "history_freeze = resnet50_freeze.fit(\n",
    "    train_images, train_labels,  epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=(val_images, val_labels), validation_steps=val_steps_per_epoch,\n",
    "    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col')\n",
    "ax[0, 0].set_title(\"loss\")\n",
    "ax[0, 1].set_title(\"val-loss\")\n",
    "ax[1, 0].set_title(\"acc\")\n",
    "ax[1, 1].set_title(\"val-acc\")\n",
    "ax[2, 0].set_title(\"top5-acc\")\n",
    "ax[2, 1].set_title(\"val-top5-acc\")\n",
    "\n",
    "ax[0, 0].plot(history_freeze.history['loss'])\n",
    "ax[0, 1].plot(history_freeze.history['val_loss'])\n",
    "ax[1, 0].plot(history_freeze.history['sparse_categorical_accuracy'])\n",
    "ax[1, 1].plot(history_freeze.history['val_sparse_categorical_accuracy'])\n",
    "ax[2, 0].plot(history_freeze.history['sparse_top_5_categorical_accuracy'])\n",
    "ax[2, 1].plot(history_freeze.history['val_sparse_top_5_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully freezing the feature extractor, we achieved a new high in terms of accuracy! With ~75% top-1 / ~92% top-5 accuracy, we are now far from the original ~37% top-1 / ~64% top-5 accuracy obtained with the same model, without transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with Fine-tuned Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we will define the exact same ResNet-50 network with pre-trained layers. However this time, we will not completely freeze its feature-extractor component, in order to _fine-tune_ the latest, higher-level convolutional layers. As we explained in Chapter 4, this fine-tuning can benefit the new classifier which may learn to extract more task-relevant features _(fine-tuning is recommend only if the training dataset is big enough to avoid over-fitting)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a New Classifier from Pre-trained Keras Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building the same network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_feature_extractor = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False, weights='imagenet', \n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), classes=NUM_CLASSES)\n",
    "\n",
    "features = resnet50_feature_extractor.output\n",
    "avg_pool = GlobalAveragePooling2D(data_format='channels_last')(features)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(avg_pool)\n",
    "\n",
    "resnet50_finetune = Model(resnet50_feature_extractor.input, predictions)\n",
    "# resnet50_finetune.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the idea this time is to fix the weights of the first layers, and retrain all the others. For instance, we will here fix the weights of the 3 first _macro-blocks_ (see Notebook [4-2](./ch4_notebook_2_resnet_from_scratch.ipynb) for definition) and fine-tune the 2 remaining ones while training the new final layers.\n",
    "\n",
    "Note that in practice, deciding which layers to fine-tune or not may require several trainings to compare the performance of the corresponding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet50_finetune.layers:\n",
    "    if 'res4' in layer.name:\n",
    "        # Keras developers named the layers in their ResNet implementation to explicitly \n",
    "        # identify which macro-block and block each layer belongs to.\n",
    "        # If we reach a layer which has a name starting by 'resnet4', it means we reached \n",
    "        # the 4th macro-block / we are done with the 3rd one:\n",
    "        break\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        layer.trainable = False\n",
    "        print(\"Layer {}: trainable = False\".format(layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start from the beginning the data iteration, we re-instantiate the input pipelines (same parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, _, _ = tiny_imagenet(\n",
    "    phase='train', shuffle=True, batch_size=batch_size, num_epochs=num_epochs, wrap_for_estimator=False,\n",
    "    augmentation_fn=_training_augmentation_fn, root_folder=TINY_IMAGENET_ROOT_FOLDER,\n",
    "    resize_to=[IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "val_images, val_labels, _, _ = tiny_imagenet(\n",
    "    phase='val', shuffle=False, batch_size=batch_size, num_epochs=None, wrap_for_estimator=False,\n",
    "    augmentation_fn=None, root_folder=TINY_IMAGENET_ROOT_FOLDER,\n",
    "    resize_to=[IMG_HEIGHT, IMG_WIDTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training takes place as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set a smaller learning rate for the fine-tuning:\n",
    "optimizer = tf.keras.optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_dir = './models/resnet_keras_app_transfer_learning_finetune'\n",
    "callbacks = [\n",
    "    # Callback to log the graph, losses and metrics into TensorBoard:\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=model_dir, histogram_freq=0, write_graph=True),\n",
    "    # Callback to save the model (e.g., every 5 epochs), specifying the epoch and val-loss in the filename:\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(model_dir, 'weights-epoch{epoch:02d}-loss{val_loss:.2f}.h5'), period=5)\n",
    "]\n",
    "\n",
    "resnet50_finetune.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "                          metrics=['sparse_categorical_accuracy', sparse_top_5_categorical_accuracy])\n",
    "history_finetune = resnet50_finetune.fit(\n",
    "    train_images, train_labels, epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=(val_images, val_labels), validation_steps=val_steps_per_epoch,\n",
    "    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col') # add parameter `sharey='row'` for a more direct comparison\n",
    "ax[0, 0].set_title(\"loss\")\n",
    "ax[0, 1].set_title(\"val-loss\")\n",
    "ax[1, 0].set_title(\"acc\")\n",
    "ax[1, 1].set_title(\"val-acc\")\n",
    "ax[2, 0].set_title(\"top5-acc\")\n",
    "ax[2, 1].set_title(\"val-top5-acc\")\n",
    "\n",
    "histories = {'freezing': history_freeze 'fine-tuning': history_finetune}\n",
    "lines, labels = [], []\n",
    "for config_name in histories:\n",
    "    history = histories[config_name]\n",
    "    ax[0, 0].plot(history.history['loss'])\n",
    "    ax[0, 1].plot(history.history['val_loss'])\n",
    "    ax[1, 0].plot(history.history['sparse_categorical_accuracy'])\n",
    "    ax[1, 1].plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    ax[2, 0].plot(history.history['sparse_top_5_categorical_accuracy'])\n",
    "    line = ax[2, 1].plot(history.history['val_sparse_top_5_categorical_accuracy'])\n",
    "    lines.append(line[0])\n",
    "    labels.append(config_name)\n",
    "\n",
    "fig.legend(lines, labels, loc='center right', borderaxespad=0.1)\n",
    "plt.subplots_adjust(right=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = max(history_finetune.history['val_sparse_categorical_accuracy']) * 100\n",
    "best_val_top5 = max(history_finetune.history['val_sparse_top_5_categorical_accuracy']) * 100\n",
    "\n",
    "print('Best val acc:  {:2.2f}%'.format(best_val_acc))\n",
    "print('Best val top5: {:2.2f}%'.format(best_val_top5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
