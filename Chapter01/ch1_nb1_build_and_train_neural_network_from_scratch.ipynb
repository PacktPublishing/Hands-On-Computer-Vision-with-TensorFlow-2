{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"border: 1px solid #e7692c; border-left: 15px solid #e7692c; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #e7692c\">Tip.</strong> <a style=\"color: #000000;\" href=\"https://nbviewer.jupyter.org/github/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/blob/master/Chapter01/ch1_nb1_build_and_train_neural_network_from_scratch.ipynb\" title=\"View with Jupyter Online\">Click here to view this notebook on <code>nbviewer.jupyter.org</code></a>. \n",
    "    <br/>These notebooks are better read there, as Github default viewer ignores some of the formatting and interactive content.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "    <tr style=\"vertical-align: top; padding: 0; margin: 0;\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #363636; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #e7692c;\">Hands-on</span> Computer Vision with TensorFlow 2</span><br/>by <em>Eliot Andres</em> & <em>Benjamin Planche</em> (Packt Pub.)</strong><br/><br/>\n",
    "        <strong>> Chapter 1: Computer Vision and Neural Networks</strong><br/>\n",
    "    </p>\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #e7692c;\">Notebook 1:</small><br/>Building and Training a Neural Network <br/>from Scratch</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #363636; text-align:justify; padding: 0 10px;\">\n",
    "    In the introductory chapter of the book, we presented what <em>computer vision</em> and <em>deep learning</em> are, and how <em>neural</em> networks work. To illustrate the latter, we described how to <strong>build a simple neural network from scratch</strong>, and how to <strong>apply it to a classification task</strong>.\n",
    "    <br/><br/>In this first notebook, we will therefore detail the related code snippets and results from the book.\n",
    "</p>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #e7692c; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #e7692c;\">Tip.</strong> The notebooks shared on this git repository illustrate some of notions from the book \"<em><strong>Hands-on Computer Vision with TensorFlow 2</strong></em>\" written by Eliot Andres and Benjamin Planche and published by Packt. If you enjoyed the insights shared here, <strong>please consider acquiring the book!</strong>\n",
    "<br/><br/>\n",
    "The book provides further guidance for those eager to learn about computer vision and to harness the power of TensorFlow 2 and Keras to build performant recognition systems for object detection, segmentation, video processing, smartphone applications, and more.</p>\n",
    "        </td>\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; width: 250px;\">\n",
    "    <a href=\"https://www.packtpub.com\" title=\"Buy on Packt!\">\n",
    "        <img src=\"../banner_images/book_cover.png\" width=250>\n",
    "    </a>\n",
    "    <p style=\"background: #e7692c; color:#ffffff; padding: 10px; text-align:justify;\"><strong>Leverage deep learning to create powerful image processing apps with TensorFlow 2 and Keras. <br/></strong>Get the book for more insights!</p>\n",
    "    <ul style=\"height: 32px; white-space: nowrap; text-align: center; margin: 0px; padding: 0px; padding-top: 10px;\">\n",
    "    <li style=\"display: inline-block; height: 100%; vertical-align: middle; float: left; margin: 5px; padding: 0px;\">\n",
    "        <a href=\"https://www.packtpub.com\" title=\"Get the book on Amazon!\">\n",
    "        <img style=\"vertical-align: middle; max-width: 72px; max-height: 32px;\" src=\"../banner_images/logo_amazon.png\" width=\"75px\">\n",
    "        </a>\n",
    "    </li>\n",
    "    <li style=\"display: inline-block; height: 100%; vertical-align: middle; float: left; margin: 5px; padding: 0px;\">\n",
    "        <a href=\"https://www.packtpub.com\" title=\"Get your Packt book!\">\n",
    "        <img style=\"vertical-align: middle; max-width: 72px; max-height: 32px;\" src=\"../banner_images/logo_packt.png\" width=\"75px\">\n",
    "        </a>\n",
    "    </li>\n",
    "    <li style=\"display: inline-block; height: 100%; vertical-align: middle; float: left; margin: 5px; padding: 0px;\">\n",
    "        <a href=\"https://www.packtpub.com\" title=\"Get the book on O'Reilly Safari!\">\n",
    "        <img style=\"vertical-align: middle; max-width: 72px; max-height: 32px;\" src=\"../banner_images/logo_oreilly.png\" width=\"75px\">\n",
    "        </a>\n",
    "    </li>\n",
    "    </ul>\n",
    "        </td>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy    # Uncomment this line if the numpy library has not bee installed yet.\n",
    "import numpy as np      # We use numpy to make vector and matrix computations easy.\n",
    "np.random.seed(42)      # Fixing the seed for the random number generation, to get reproducable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At the Beginning: the Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the comparison to the original, biological neurons, we explained in the book how scientists developed a simple _model_ to simulate their behavior[$^1$](#ref).\n",
    "\n",
    "The code below was provided in the chapter, along the mathematical explanations for each operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \"\"\"\n",
    "    A simple artificial neuron, processing an input vector and returning a corresponding activation.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        activation_function (callable): The activation function defining this neuron.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (float): The bias value, added to the weighted sum.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation_function):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (e.g., using a simplistic \n",
    "        # uniform distribution between -1 and 1):\n",
    "        self.W = np.random.uniform(size=num_inputs, low=-1., high=1.)\n",
    "        self.b = np.random.uniform(size=1, low=-1., high=1.)\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input signal through the neuron, returning its activation value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(1, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return self.activation_function(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class represents a simple artificial neuron, able to receive a vector of input values, to merge and process them before returning an activation value. We will now demonstrate how this model can be used.\n",
    "\n",
    "***Note:*** This class can also be found in [neuron.py](neuron.py).\n",
    "\n",
    "First, we instantiate our neuron. Let us create a ***perceptron*** (c.f. Chapter 1) taking 2 input values and using the _step_ function for computing its *activation*. Its weights and bias values are randomly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's random weights = [-0.25091976  0.90142861  0.46398788] , and random bias = [0.19731697]\n"
     ]
    }
   ],
   "source": [
    "# Perceptron input size:\n",
    "input_size = 3\n",
    "\n",
    "# Step function (returns 0 if y <= 0, or 1 if y > 0):\n",
    "step_function = lambda y: 0 if y <= 0 else 1\n",
    "\n",
    "# Instantiating the perceptron:\n",
    "perceptron = Neuron(num_inputs=input_size, activation_function=step_function)\n",
    "print(\"Perceptron's random weights = {} , and random bias = {}\".format(perceptron.W, perceptron.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate a random input vector of 3 values (i.e. a column-vector of (shape = `(1, 3)`), to be fed to our neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector : [[0.15601864 0.15599452 0.05808361]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(input_size).reshape(1, input_size)\n",
    "print(\"Input vector : {}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed our perceptron with this input and display the corresponding activation. We invite our readers to try different inputs or edit the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's output value given `x` : 1\n"
     ]
    }
   ],
   "source": [
    "y = perceptron.forward(x)\n",
    "print(\"Perceptron's output value given `x` : {}\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `Neuron` class, we implemented the mathematical model for neurons proposed by early A.I. scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layering Neurons Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the homonym section of Chapter 1, we presented how neurons can be organized into ***layers***. We introduced the following model, mathematically wrapping together the operations done by such a neural layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        layer_size (int): The output vector size / number of neurons in the layer.\n",
    "        activation_function (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
    "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
    "        derivated_activation_function (callable): The corresponding derivated function for backpropagation.\n",
    "        dL_dW (ndarray): The derivative of the loss, with respect to the weights W.\n",
    "        dL_db (ndarray): The derivative of the loss, with respect to the bias b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.x, self.y = None, None\n",
    "        self.dL_dW, self.dL_db = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer, returning its activation vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_function(z)\n",
    "        self.x = x  # (we store the input and output values for back-propagation)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss, computing all the derivatives, storing those w.r.t. the layer parameters,\n",
    "        and returning the loss w.r.t. its inputs for further propagation.\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the layer's output (dL/dy = l'_{k+1}).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the layer's input (dL/dx).\n",
    "        \"\"\"\n",
    "        dy_dz = self.derivated_activation_function(self.y)  # = f'\n",
    "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
    "        dz_dw = self.x.T\n",
    "        dz_dx = self.W.T\n",
    "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
    "\n",
    "        # Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\n",
    "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
    "        self.dL_db = np.dot(dz_db, dL_dz)\n",
    "\n",
    "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
    "        dL_dx = np.dot(dL_dz, dz_dx)\n",
    "        return dL_dx\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the layer's parameters, using the stored derivative values.\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.W -= epsilon * self.dL_dW\n",
    "        self.b -= epsilon * self.dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** This class can also be found in [fully_connected_layer.py](fully_connected_layer.py). \n",
    "\n",
    "Once again, let us quickly show how this *layer* can be used to process input values, one by one or stacked together into ***batches***. We instantiate a layer of 3 neurons (so 3 output values), taking 2 input values and applying this time the ***ReLU*** (Rectified Linear Unit) function for the activations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size    = 2\n",
    "num_neurons   = 3\n",
    "relu_function = lambda y: np.maximum(y, 0)\n",
    "\n",
    "layer = FullyConnectedLayer(num_inputs=input_size, layer_size=num_neurons, activation_function=relu_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate 2 random input vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector #1: [[-0.72101228 -0.4157107 ]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "print(\"Input vector #1: {}\".format(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector #2: [[-0.26727631 -0.08786003]]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "print(\"Input vector #2: {}\".format(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our layer can either process them separetely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer's output value given `x1` : [[0.         0.4593046  1.61941647]]\n"
     ]
    }
   ],
   "source": [
    "y1 = layer.forward(x1)\n",
    "print(\"Layer's output value given `x1` : {}\".format(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer's output value given `x2` : [[0.         0.73048436 1.05288999]]\n"
     ]
    }
   ],
   "source": [
    "y2 = layer.forward(x2)\n",
    "print(\"Layer's output value given `x2` : {}\".format(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer's output value given `[x1, x2]` :\n",
      "[[0.         0.4593046  1.61941647]\n",
      " [0.         0.73048436 1.05288999]]\n"
     ]
    }
   ],
   "source": [
    "x12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\n",
    "y12 = layer.forward(x12)\n",
    "print(\"Layer's output value given `[x1, x2]` :\\n{}\".format(y12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Complete Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of neural layers is to be stacked together to form a ***neural network*** able to perform non-linear predictions.\n",
    "\n",
    "Applying a ***gradient descent***, such a network can be trained to perform correct predictions (c.f. theory in Chapter 1). But for that, we need a ***loss*** function to evaluate the performance of the network (c.f. ***L2*** or ***cross-entropy*** losses introduced in Chapter 1), and we need to know how to *derive* all the operations performed by the network, to compute and propagate the gradients.\n",
    "\n",
    "In this section, we will present how a simple fully-connected neural network can be built. Let us assume we want our network to use the *sigmoid* function for the activation. We need to implement that function _and_ its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):             # sigmoid function\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def derivated_sigmoid(y):   # sigmoid derivative function\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to build a neural network for classification. We would use the *L2* or *cross-entropy* loss previously introduced. We should also implement them, along their derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_L2(pred, target):             # L2 loss function\n",
    "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
    "\n",
    "\n",
    "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
    "    return 2 * (pred - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
    "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
    "\n",
    "\n",
    "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
    "    return (pred - target) / (pred * (1 - pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the book, we should now connect everything together, building a class able to connect multiple neural layers together, able to to feed-forward data through these layers and back-propagate the loss' gradients for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    \"\"\"A simple fully-connected NN.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        num_outputs (int): The output vector size.\n",
    "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
    "        activation_function (callable): The activation function for all the layers\n",
    "        derivated_activation_function (callable): The derivated activation function\n",
    "        loss_function (callable): The loss function to train this network\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
    "    Attributes:\n",
    "        layers (list): The list of layers forming this simple network.\n",
    "        loss_function (callable): The loss function to train this network.\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
    "        super().__init__()\n",
    "        # We build the list of layers composing the network, according to the provided arguments:\n",
    "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], \n",
    "                                activation_function, derivated_activation_function)\n",
    "            for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layers, returning the output vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
    "        Returns:\n",
    "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        for layer in self.layers: # from the input layer to the output one\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Compute the output corresponding to input `x`, and return the index of the largest \n",
    "        output value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
    "        Returns:\n",
    "            best_class (int): The predicted class ID.\n",
    "        \"\"\"\n",
    "        estimations = self.forward(x)\n",
    "        best_class = np.argmax(estimations)\n",
    "        return best_class\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # from the output layer to the input one\n",
    "            dL_dy = layer.backward(dL_dy)\n",
    "        return dL_dy\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
    "        to be called before).\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:             # the order doesn't matter here\n",
    "            layer.optimize(epsilon)\n",
    "\n",
    "    def evaluate_accuracy(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy of the network \n",
    "                              (= number of correct predictions/dataset size).\n",
    "        \"\"\"\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects / len(X_val)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_train (ndarray): The input training dataset.\n",
    "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "            batch_size (int): The mini-batch size.\n",
    "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
    "            learning_rate (float): The learning rate to scale the derivatives.\n",
    "            print_frequency (int): Frequency to print metrics (in epochs).\n",
    "        Returns:\n",
    "            losses (list): The list of training losses for each epoch.\n",
    "            accuracies (list): The list of validation accuracy values for each epoch.\n",
    "        \"\"\"\n",
    "        num_batches_per_epoch = len(X_train) // batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        losses, accuracies = [], []\n",
    "        for i in range(num_epochs): # for each training epoch\n",
    "            epoch_loss = 0\n",
    "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
    "                # Get batch:\n",
    "                batch_index_begin = b * batch_size\n",
    "                batch_index_end = batch_index_begin + batch_size\n",
    "                x = X_train[batch_index_begin: batch_index_end]\n",
    "                targets = y_train[batch_index_begin: batch_index_end]\n",
    "                # Optimize on batch:\n",
    "                predictions = y = self.forward(x)  # forward pass\n",
    "                L = self.loss_function(predictions, targets)  # loss computation\n",
    "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
    "                self.backward(dL_dy)  # back-propagation pass\n",
    "                self.optimize(learning_rate)  # optimization of the NN\n",
    "                epoch_loss += L\n",
    "\n",
    "            # Logging training loss and validation accuracy, to follow the training:\n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            losses.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "                    i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** This class can also be found in [simple_network.py](simple_network.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our Network to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using untrained perceptrons and layers on random inputs is however a bit dull. In this final section of the notebook, we instantiate and train our simple model to ***classify images of hand-written digits***. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we will use the the [MNIST dataset](http://yann.lecun.com/exdb/mnist) presented in the book[$^2$](#ref) (Yann LeCun and Corinna Cortes hold all copyrights for this dataset).\n",
    "\n",
    "Before implementing a solution, we should prepare the data, loading the MNIST images for training and testing methods. For simplicity, we will use the Python module [`mnist`](https://github.com/datapythonista/mnist) developed by [Marc Garcia](https://github.com/datapythonista), and already installed in this chapter's directory (see [`./mnist/`](mnist/__init__.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install matplotlib  # Uncomment and run if matplotlib is not installed yet.\n",
    "import matplotlib          # We use this package to visualize some data and results\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mnist` module makes it simple to load the training and testing data (images and their labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test,  y_test  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number and size of the training/testing samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. we have 60,000 training samples and 10,000 testing one, with each sample an image of $28 \\times 28$ pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the data, for instance using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABGJJREFUeJzt3b0udGsYgGGz85HQaYhEodFJaPV0jlPjNDRaREdNJ6KhMPsILLa1Z8Y393W1z6yf5s5TvDNMptPpCtDzz6JfAFgM8UOU+CFK/BAlfogSP0SJH6LED1Hih6g/c36erxPC7E2+8yGbH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8EDXvf9HNX+bg4GBw/v7+Pji/uLj4dHZ4ePijd+L/YfNDlPghSvwQJX6IEj9EiR+ixA9Rzvnjbm9vB+d3d3eD8+l0Ojh/fHz8dOacf7FsfogSP0SJH6LED1HihyjxQ5T4Ico5f9xX5/Rfzfl72fwQJX6IEj9EiR+ixA9R4ocoR31xNzc3i34FFsTmhyjxQ5T4IUr8ECV+iBI/RIkfopzzxznn77L5IUr8ECV+iBI/RIkfosQPUeKHKOf8cff394t+BRbE5oco8UOU+CFK/BAlfogSP0SJH6Kc88ddXV2Nun5tbW1wvr29Per+zI7ND1HihyjxQ5T4IUr8ECV+iBI/RDnnZ5SNjY3B+dHR0ZzehP/K5oco8UOU+CFK/BAlfogSP0Q56ltyLy8vg/O3t7dR99/a2hp1PYtj80OU+CFK/BAlfogSP0SJH6LED1HO+Zfc9fX14Pzp6WnU/U9PT0ddz+LY/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlN/zM8re3t6iX4EfsvkhSvwQJX6IEj9EiR+ixA9R4oco5/yMMplMFv0K/JDND1HihyjxQ5T4IUr8ECV+iHLUt+TOz89nev/j4+OZ3p/ZsfkhSvwQJX6IEj9EiR+ixA9R4oco5/xL7urqatGvwC9l80OU+CFK/BAlfogSP0SJH6LED1GT6XQ6z+fN9WGsrGxubg7On5+fB+e7u7uD84eHh8H56urq4JyZ+NbfU7f5IUr8ECV+iBI/RIkfosQPUeKHKL/nZ9BX3xNwjv/3svkhSvwQJX6IEj9EiR+ixA9RjvqWwOXl5aezl5eXUfc+OzsbdT2/l80PUeKHKPFDlPghSvwQJX6IEj9EOedfAq+vr5/OPj4+Rt375ORk1PX8XjY/RIkfosQPUeKHKPFDlPghSvwQ5Zx/Cezs7Hw6W1tbG7x2fX19cL6/v/+jd+L3s/khSvwQJX6IEj9EiR+ixA9R4oeoyXQ6nefz5vowiJp850M2P0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IerPnJ/3rX8dDMyezQ9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9E/QshZlVoOGDWmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_idx = np.random.randint(0, X_test.shape[0])\n",
    "plt.imshow(X_test[img_idx], cmap=matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[img_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our images match their ground-truth label, which is good news!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as our network only accepts column vectors, we need to _flatten_ the images into 1D vectors, i.e. vectors of shape `(1, 784)` (since $28 \\times 28 = 784$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let us have a look at our pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values between 0 and 255\n"
     ]
    }
   ],
   "source": [
    "print(\"Pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are normal integer values for images with 8 bits per channel (`uint8`)... These values may be however too big for some of our operations. For instance, given a too big input value, our sigmoid may return `nan` (\"_not a number_\") because of the exponential function it uses, which may \"overflow\" with a large input value.\n",
    "\n",
    "It is thus customary to *normalize* the input data, i.e. to scale the values between 0 and 1 (or -1 and 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized pixel values between 0.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to compute the loss, we need to ***one-hot*** the labels, e.g. converting the label `4` into `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(num_classes)[y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to prepare the classifier itself. Let us use our `SimpleNetwork` class and instantiate a network with 2 hidden layers, taking a flattened image as input and returning a 10-value vector representing its belief the image belongs to each of the class (the highter the value,the stronger the belief): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how our network performs (computing its *loss* over the training set, and its *accuracy* over the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained : training loss = 4.436700 | val accuracy = 12.19%\n"
     ]
    }
   ],
   "source": [
    "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
    "loss_untrained = mnist_classifier.loss_function(predictions, y_train)   # loss computation\n",
    "\n",
    "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "    loss_untrained, accuracy_untrained * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... This is a really poor performance... But as we know from the book, this is to be expected: we have yet to train our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching our Network to Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things finally get interesting. As the whole training procedure is already explained and implemented, we simply have to launch it (note: the training takes minutes/hours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: training loss = 1.096978 | val accuracy = 19.10%\n",
      "Epoch   20: training loss = 0.252953 | val accuracy = 84.89%\n",
      "Epoch   40: training loss = 0.177532 | val accuracy = 88.92%\n",
      "Epoch   60: training loss = 0.146596 | val accuracy = 90.50%\n",
      "Epoch   80: training loss = 0.128075 | val accuracy = 91.29%\n",
      "Epoch  100: training loss = 0.114770 | val accuracy = 92.01%\n",
      "Epoch  120: training loss = 0.104585 | val accuracy = 92.39%\n",
      "Epoch  140: training loss = 0.096663 | val accuracy = 92.74%\n",
      "Epoch  160: training loss = 0.090176 | val accuracy = 93.06%\n",
      "Epoch  180: training loss = 0.084631 | val accuracy = 93.22%\n",
      "Epoch  200: training loss = 0.079820 | val accuracy = 93.46%\n",
      "Epoch  220: training loss = 0.075625 | val accuracy = 93.66%\n",
      "Epoch  240: training loss = 0.071941 | val accuracy = 93.72%\n",
      "Epoch  260: training loss = 0.068681 | val accuracy = 93.93%\n",
      "Epoch  280: training loss = 0.065795 | val accuracy = 94.12%\n",
      "Epoch  300: training loss = 0.063194 | val accuracy = 94.28%\n",
      "Epoch  320: training loss = 0.060833 | val accuracy = 94.38%\n",
      "Epoch  340: training loss = 0.058652 | val accuracy = 94.44%\n",
      "Epoch  360: training loss = 0.056625 | val accuracy = 94.50%\n",
      "Epoch  380: training loss = 0.054748 | val accuracy = 94.48%\n",
      "Epoch  400: training loss = 0.052997 | val accuracy = 94.54%\n",
      "Epoch  420: training loss = 0.051355 | val accuracy = 94.51%\n",
      "Epoch  440: training loss = 0.049836 | val accuracy = 94.60%\n",
      "Epoch  460: training loss = 0.048432 | val accuracy = 94.69%\n",
      "Epoch  480: training loss = 0.047126 | val accuracy = 94.76%\n",
      "Epoch  499: training loss = 0.045963 | val accuracy = 94.83%\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=30, num_epochs=500)\n",
    "# note: Reduce the batch size and/or number of epochs if your computer can't \n",
    "#       handle the computations / takes too long.\n",
    "#       Remember, numpy also uses the CPU, not GPUs as modern Deep Learning \n",
    "#       libraries do, hence the lack of computational performance here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost **95%** of accuracy! This is much better. Congratulations, we implemented and trained our first neural network classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the evolution of the loss and accuracy during the training, to better visualize the evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses, accuracies = [loss_untrained] + losses, [accuracy_untrained] + accuracies\n",
    "fig, ax_loss = plt.subplots()\n",
    "\n",
    "color = 'red'\n",
    "ax_loss.set_xlim([0, 510])\n",
    "ax_loss.set_xlabel('Epochs')\n",
    "ax_loss.set_ylabel('Training Loss', color=color)\n",
    "ax_loss.plot(losses, color=color)\n",
    "ax_loss.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax_acc = ax_loss.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'blue'\n",
    "ax_acc.set_xlim([0, 510])\n",
    "ax_acc.set_ylim([0, 1])\n",
    "ax_acc.set_ylabel('Val Accuracy', color=color)\n",
    "ax_acc.plot(accuracies, color=color)\n",
    "ax_acc.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our network *converged* quite fast, though the accuracy slowly kept increasing. It looks even like the accuracy could still go up a bit with some further training iterations... We leave it to our readers to check. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before concluding this notebook, let us verify how our network is now performing on our random test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1; Correct class: 1\n"
     ]
    }
   ],
   "source": [
    "# We use `np.expand_dims(x, 0)` to simulate a batch (transforming the image shape\n",
    "# from (784,) to (1, 784)):\n",
    "predicted_class = mnist_classifier.predict(np.expand_dims(X_test[img_idx], 0))\n",
    "print('Predicted class: {}; Correct class: {}'.format(predicted_class, y_test[img_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this first notebook, we illustrated the main notions of Chapter 1, starting with the implementation of a single artificial neuron and ending up with a network compose of several thousand neurons, able to classify digits with a decent accuracy.\n",
    "\n",
    "Training this network was however a slow process. In the next chapter and related notebooks, we will finally get started with TensorFlow 2 and Keras, to easily scale up our use-cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rosenblatt, F., 1958. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review 65, 386.\n",
    "2. LeCun, Y., Cortes, C., Burges, C., 2010. MNIST handwritten digit database. AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2, 18."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
